# Databricks データウェアハウス構築演習 - Northwindデータセット

## 概要
このドキュメントは、Microsoft Northwindデータセット（架空の貿易会社のデータ）を使用したDatabricksでのデータウェアハウス構築演習です。顧客、注文、在庫、従業員などのビジネスデータを使用して、実践的なデータ分析基盤を構築します。

## 前提条件
- Databricks Community Editionにアクセス可能
- 以下のCSVファイルがDatabricksボリュームにアップロード済み
  - categories.csv
  - customers.csv
  - employees.csv
  - orders.csv
  - order_details.csv
  - products.csv
  - suppliers.csv
  - shippers.csv
  - regions.csv
  - territories.csv
  - employee_territories.csv

## データ構造の理解

### テーブル構成
1. **categories** - 商品カテゴリ（飲料、調味料、菓子類など）
2. **customers** - 顧客情報（会社名、連絡先、所在地）
3. **employees** - 従業員情報（氏名、役職、上司）
4. **orders** - 注文情報（注文日、配送先、運賃）
5. **order_details** - 注文明細（商品、数量、単価、割引）
6. **products** - 商品情報（商品名、在庫数、単価）
7. **suppliers** - 仕入先情報
8. **shippers** - 配送業者情報
9. **regions** / **territories** / **employee_territories** - 地域・担当エリア情報

---

## 演習1: ブロンズレイヤー - データ取り込み

### 目的
CSVファイルからDelta Lake形式のブロンズテーブルを作成します。

### タスク
1. ボリュームパスを確認し、CSVファイルの場所を特定する
2. 以下のテーブルをDelta形式でブロンズレイヤーに作成する
   - bronze.categories
   - bronze.customers
   - bronze.employees
   - bronze.orders
   - bronze.order_details
   - bronze.products
   - bronze.suppliers
   - bronze.shippers

### ヒント
```python
# CSVファイルの読み込み例
df = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("/Volumes/<catalog>/<schema>/<volume>/categories.csv")

# Deltaテーブルとして保存
df.write.format("delta").mode("overwrite").saveAsTable("bronze.categories")
```

### 確認事項
- すべてのテーブルが正しく作成されているか
- データ型が適切に推論されているか
- レコード数が元のCSVと一致しているか

---

## 演習2: シルバーレイヤー - データクレンジング

### 目的
ブロンズレイヤーのデータをクレンジングし、分析に適した形式に変換します。

### タスク

#### 2-1: 顧客テーブルのクレンジング
- NULL値の処理（regionやfaxなど）
- 国名の標準化
- 不要なカラムの削除

#### 2-2: 注文テーブルの変換
- 日付型への変換（orderDate, requiredDate, shippedDate）
- 配送遅延フラグの追加（shippedDate > requiredDate）
- freight（運賃）のDECIMAL型への変換

#### 2-3: 注文明細テーブルの拡張
- 行ごとの売上金額の計算（unitPrice * quantity * (1 - discount)）
- 割引適用フラグの追加

#### 2-4: 商品テーブルの整備
- 在庫切れフラグの追加（unitsInStock = 0）
- 生産終了フラグの確認（discontinued）

### ヒント
```python
from pyspark.sql.functions import col, when, to_date

# 日付変換とフラグ追加の例
orders_silver = spark.table("bronze.orders") \
    .withColumn("order_date", to_date(col("orderDate"))) \
    .withColumn("shipped_date", to_date(col("shippedDate"))) \
    .withColumn("required_date", to_date(col("requiredDate"))) \
    .withColumn("is_delayed", when(col("shipped_date") > col("required_date"), True).otherwise(False))
```

### 確認事項
- データ型が適切に変換されているか
- 計算列が正しく作成されているか
- NULL値が適切に処理されているか

---

## 演習3: ゴールドレイヤー - ビジネス分析用マート

### 目的
分析用の集約テーブルやビジネスメトリクスを作成します。

### タスク

#### 3-1: 売上分析マートの作成
以下のディメンションで売上を集約した`gold.sales_summary`を作成：
- 年月別
- 顧客別
- 商品別
- カテゴリ別
- 国別

含めるべきメトリクス：
- 売上金額合計
- 注文件数
- 注文商品数合計
- 平均割引率

#### 3-2: 顧客分析マートの作成
`gold.customer_metrics`を作成：
- 顧客ごとの総購入金額
- 注文回数
- 初回注文日・最終注文日
- 平均注文金額
- 顧客ランク（売上高に基づく）

#### 3-3: 在庫分析マートの作成
`gold.inventory_status`を作成：
- 商品ごとの現在在庫数
- 発注済み数量
- 再発注レベル
- 在庫回転率（売上数量 / 平均在庫数）
- 在庫アラート（在庫 < 再発注レベル）

#### 3-4: 従業員パフォーマンスマート
`gold.employee_performance`を作成：
- 従業員ごとの担当注文件数
- 担当売上金額
- 平均処理日数（注文日から発送日まで）
- 月別パフォーマンス推移

### ヒント
```python
from pyspark.sql.functions import sum, count, avg, year, month, min, max

# 売上集計の例
sales_summary = spark.table("silver.order_details") \
    .join(spark.table("silver.orders"), "orderID") \
    .join(spark.table("silver.products"), "productID") \
    .join(spark.table("silver.categories"), "categoryID") \
    .groupBy(
        year("order_date").alias("year"),
        month("order_date").alias("month"),
        "categoryName"
    ) \
    .agg(
        sum("line_total").alias("total_sales"),
        count("orderID").alias("order_count"),
        sum("quantity").alias("total_quantity"),
        avg("discount").alias("avg_discount")
    )

sales_summary.write.format("delta").mode("overwrite").saveAsTable("gold.sales_summary")
```

### 確認事項
- 集約結果が正しいか
- ビジネスロジックが適切に実装されているか
- パフォーマンスが許容範囲内か

---

## 演習4: SQLによる高度な分析

### タスク

#### 4-1: RFM分析
顧客をRecency（最新購入日）、Frequency（購入頻度）、Monetary（購入金額）で分類：
```sql
-- RFMスコアを計算し、顧客をセグメント化する
-- ヒント: NTILE関数を使用してスコアリング
```

#### 4-2: 商品推薦分析
よく一緒に購入される商品ペアを発見：
```sql
-- 同じ注文に含まれる商品の組み合わせを分析
-- ヒント: order_detailsを自己結合
```

#### 4-3: 売上トレンド分析
前年同月比の売上成長率を計算：
```sql
-- 年月別の売上を前年同月と比較
-- ヒント: LAG関数やWINDOW関数を使用
```

#### 4-4: ABC分析
商品を売上貢献度でランク付け（上位20%、中位30%、下位50%）：
```sql
-- パレートの法則を適用した在庫管理指標
-- ヒント: 累積売上比率を計算
```

---

## 演習5: データ品質チェックとモニタリング

### タスク

#### 5-1: データ品質ルールの実装
以下のチェックを実装：
- 注文の整合性チェック（orderとorder_detailsの件数一致）
- 参照整合性チェック（外部キー制約の検証）
- NULL値の監視
- 日付の妥当性チェック（未来日付や異常値）

#### 5-2: データプロファイリング
各テーブルに対して：
- レコード数
- NULL値の割合
- ユニーク値の数
- 最小値・最大値・平均値

#### 5-3: アラート設定
以下の条件でアラートを設定：
- 在庫が再発注レベル以下の商品
- 配送が大幅に遅延している注文
- 異常に高額な注文

---

## 演習6: パフォーマンス最適化

### タスク

#### 6-1: パーティショニング戦略
- ordersテーブルを年月でパーティション化
- order_detailsテーブルの最適なパーティション戦略を検討

#### 6-2: Z-Orderingの適用
頻繁にフィルタされるカラムに対してZ-Orderingを適用：
```python
# 例: 国と顧客IDでZ-Ordering
spark.sql("OPTIMIZE gold.sales_summary ZORDER BY (country, customerID)")
```

#### 6-3: キャッシング戦略
よく使用されるマスターテーブルをキャッシュ：
```python
spark.table("silver.products").cache()
spark.table("silver.customers").cache()
```

---

## 演習7: ダッシュボード用ビューの作成

### タスク

#### 7-1: 経営ダッシュボード用ビュー
KPI表示用のビューを作成：
- 今月の売上・前月比・前年同月比
- 上位顧客トップ10
- 売上上位商品トップ10
- 国別売上分布

#### 7-2: 営業担当者用ビュー
従業員ごとのパフォーマンス表示：
- 担当顧客数
- 月別売上推移
- 達成率（目標との比較）

#### 7-3: 在庫管理用ビュー
在庫担当者向けの情報：
- 在庫切れ・在庫不足商品リスト
- 回転率の低い商品
- カテゴリ別在庫状況

---

## 発展課題

### 課題1: リアルタイムストリーミング（想定シナリオ）
新規注文がストリームで流入する想定で、Structured Streamingを使用したパイプラインを設計してください。

### 課題2: 機械学習との統合
- 顧客の離脱予測モデルの構築
- 商品レコメンデーションエンジンの構築
- 需要予測モデルの構築

### 課題3: データカタログとガバナンス
- Unity Catalogを使用したデータガバナンスの実装
- 個人情報（PII）のマスキング
- アクセス制御の設定

### 課題4: 自動化とオーケストレーション
- Databricks Workflowsを使用したETLパイプラインの自動化
- DLTパイプラインへの移行
- エラーハンドリングとリトライロジックの実装

---

## 学習ポイント

### データモデリング
- メダリオンアーキテクチャ（Bronze/Silver/Gold）の理解
- スタースキーマの設計
- ディメンションテーブルとファクトテーブルの関係

### Delta Lake
- ACIDトランザクション
- タイムトラベル機能
- スキーマエンフォースメントと進化

### パフォーマンスチューニング
- パーティショニング戦略
- Z-Ordering最適化
- キャッシング戦略

### ビジネス分析
- RFM分析
- コホート分析
- トレンド分析
- ABC分析

---

## 参考リソース

- [Databricks ドキュメント](https://docs.databricks.com/)
- [Delta Lake ドキュメント](https://docs.delta.io/)
- [PySpark SQL 関数リファレンス](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)

---

## 評価基準

### 基本レベル（演習1-3）
- ブロンズ/シルバー/ゴールドレイヤーの作成
- 基本的なデータクレンジング
- 簡単な集計マートの作成

### 中級レベル（演習4-5）
- 高度なSQL分析
- データ品質管理
- ビジネスメトリクスの実装

### 上級レベル（演習6-7、発展課題）
- パフォーマンス最適化
- 自動化とオーケストレーション
- 機械学習との統合

---

## 成果物チェックリスト

- [ ] すべてのブロンズテーブルが作成されている
- [ ] シルバーテーブルでデータがクレンジングされている
- [ ] 最低3つ以上のゴールドマートが作成されている
- [ ] SQL分析クエリが正しく動作する
- [ ] データ品質チェックが実装されている
- [ ] パフォーマンス最適化が適用されている
- [ ] ドキュメントが整備されている

---

頑張ってください！
